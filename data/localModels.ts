
import { LocalModelDB } from '../types';

export const MODEL_DB: LocalModelDB[] = [
  // --- Apple Silicon MLX Models ---
  {
    name: "Llama-3.2-3B-Instruct-MLX",
    repo: "mlx-community/Llama-3.2-3B-Instruct-4bit",
    params_b: 3.2,
    type: "LLM",
    license: "Llama Community",
    formats: ["Safetensors"],
    backend: "mlx",
    quantization: "4bit",
    apple_silicon_optimized: true,
    min_hardware: {
      cpu: "Apple Silicon",
      ram_gb: 8,
      gpu_vram_gb: 3
    },
    tasks: ["chat", "qa", "creative"],
    pinokio: false,
    text_to_video_prompt: true,
    text_to_image_prompt: true,
    description: "Highly efficient Llama 3.2 3B model optimized for Apple Silicon via MLX. Blazing fast inference on M1/M2/M3.",
    publisher: "Meta / MLX Community",
    provider: "Meta",
    libraries: ["MLX", "Safetensors", "Transformers"],
    providers: ["Groq", "Together AI", "Fireworks"]
  },
  {
    name: "DeepSeek-Coder-V2-Lite-MLX",
    repo: "mlx-community/DeepSeek-Coder-V2-Lite-Instruct-4bit-mlx",
    params_b: 16,
    type: "MoE",
    license: "DeepSeek License",
    formats: ["Safetensors"],
    backend: "mlx",
    quantization: "4bit",
    apple_silicon_optimized: true,
    min_hardware: {
      cpu: "Apple Silicon",
      ram_gb: 16,
      gpu_vram_gb: 10
    },
    tasks: ["coding", "chat", "reasoning"],
    pinokio: false,
    text_to_video_prompt: false,
    text_to_image_prompt: false,
    description: "State-of-the-art coding model (MoE) optimized for Mac. Rivals GPT-4 Turbo in coding tasks.",
    publisher: "DeepSeek / MLX Community",
    provider: "DeepSeek",
    libraries: ["MLX", "Safetensors"],
    providers: ["DeepSeek", "Together AI"]
  },
  {
    name: "Phi-3.5-Mini-Instruct-MLX",
    repo: "mlx-community/Phi-3.5-mini-instruct-4bit",
    params_b: 3.8,
    type: "LLM",
    license: "MIT",
    formats: ["Safetensors"],
    backend: "mlx",
    quantization: "4bit",
    apple_silicon_optimized: true,
    min_hardware: {
      cpu: "Apple Silicon",
      ram_gb: 8,
      gpu_vram_gb: 4
    },
    tasks: ["chat", "qa", "reasoning"],
    pinokio: false,
    text_to_video_prompt: true,
    text_to_image_prompt: true,
    description: "Microsoft's Phi-3.5 Mini, perfectly optimized for Apple Silicon. Exceptional reasoning for its size.",
    publisher: "Microsoft / MLX Community",
    provider: "Microsoft",
    libraries: ["MLX", "Transformers"],
    providers: ["Azure AI", "HuggingFace"]
  },
  {
    name: "Mistral-Nemo-12B-MLX",
    repo: "mlx-community/Mistral-Nemo-Instruct-2407-4bit",
    params_b: 12,
    type: "LLM",
    license: "Apache 2.0",
    formats: ["Safetensors"],
    backend: "mlx",
    quantization: "4bit",
    apple_silicon_optimized: true,
    min_hardware: {
      cpu: "Apple Silicon",
      ram_gb: 16,
      gpu_vram_gb: 8
    },
    tasks: ["chat", "qa", "creative", "coding"],
    pinokio: false,
    text_to_video_prompt: true,
    text_to_image_prompt: true,
    description: "Mistral Nemo 12B running natively on MLX. Large context window and strong general performance.",
    publisher: "Mistral AI / MLX Community",
    provider: "Mistral AI",
    libraries: ["MLX", "Transformers"],
    providers: ["Mistral AI", "Together AI", "Groq"]
  },

  // --- RISC-V / Edge / NPU Friendly Models ---
  {
    name: "Qwen2.5-1.5B-Instruct",
    repo: "Qwen/Qwen2.5-1.5B-Instruct-GGUF",
    params_b: 1.5,
    type: "LLM",
    license: "Apache 2.0",
    formats: ["GGUF"],
    backend: "gguf",
    quantization: "Q4_K_M",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "RISC-V",
      ram_gb: 4,
      gpu_vram_gb: 2
    },
    tasks: ["chat", "coding", "edge"],
    pinokio: true,
    text_to_video_prompt: true,
    text_to_image_prompt: true,
    description: "Ultra-lightweight model from Alibaba. Perfect for RISC-V boards (e.g., Lichee Pi, VisionFive 2) and Raspberry Pi.",
    publisher: "Qwen",
    provider: "Alibaba Cloud",
    libraries: ["GGUF", "llama.cpp"],
    providers: ["Alibaba Cloud"]
  },
  {
    name: "Llama-3.2-1B-Instruct",
    repo: "huggingface/Llama-3.2-1B-Instruct-GGUF",
    params_b: 1,
    type: "LLM",
    license: "Llama Community",
    formats: ["GGUF"],
    backend: "gguf",
    quantization: "Q4_K_M",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "ARM64",
      ram_gb: 4,
      gpu_vram_gb: 2
    },
    tasks: ["chat", "edge", "qa"],
    pinokio: true,
    text_to_video_prompt: true,
    text_to_image_prompt: true,
    description: "Meta's smallest Llama 3.2 model. Optimized for on-device deployment, mobile, and NPU acceleration.",
    publisher: "Meta",
    provider: "Meta",
    libraries: ["GGUF", "llama.cpp", "ExecuTorch"],
    providers: ["Groq", "Together AI"]
  },

  // --- Specialized OCR & Vision Models ---
  {
    name: "Florence-2-Large",
    repo: "microsoft/Florence-2-large",
    params_b: 0.77,
    type: "Vision",
    license: "MIT",
    formats: ["ONNX", "Transformers"],
    backend: "onnx",
    quantization: "FP16",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 4,
      gpu_vram_gb: 4
    },
    tasks: ["ocr", "vision", "captioning"],
    pinokio: true,
    text_to_video_prompt: false,
    text_to_image_prompt: false,
    description: "Microsoft's lightweight vision-language model. Exceptional performance on OCR and object detection tasks.",
    publisher: "Microsoft",
    provider: "Microsoft",
    libraries: ["ONNX", "Transformers"],
    providers: ["Azure AI"]
  },
  {
    name: "Qwen2-VL-7B-Instruct",
    repo: "Qwen/Qwen2-VL-7B-Instruct-GGUF",
    params_b: 7,
    type: "Multimodal",
    license: "Apache 2.0",
    formats: ["GGUF"],
    backend: "gguf",
    quantization: "Q4_K_M",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 16,
      gpu_vram_gb: 8
    },
    tasks: ["vision", "ocr", "chat", "video-analysis"],
    pinokio: true,
    text_to_video_prompt: false,
    text_to_image_prompt: false,
    description: "State-of-the-art vision model from Qwen. Can read text in images, analyze videos, and understand diagrams.",
    publisher: "Qwen",
    provider: "Alibaba Cloud",
    libraries: ["GGUF", "llama.cpp", "Transformers"],
    providers: ["Alibaba Cloud"]
  },

  // --- Text-to-Video Models ---
  {
    name: "CogVideoX-2B",
    repo: "THUDM/CogVideoX-2b",
    params_b: 2,
    type: "Video",
    license: "CogVideoX License",
    formats: ["Safetensors"],
    backend: "pytorch",
    quantization: "BF16",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "x86",
      ram_gb: 16,
      gpu_vram_gb: 12
    },
    tasks: ["text-to-video"],
    pinokio: true,
    text_to_video_prompt: false,
    text_to_image_prompt: false,
    description: "Open-weight text-to-video model by THUDM. Capable of generating coherent 6-second clips. Requires good GPU.",
    publisher: "THUDM",
    provider: "THUDM",
    libraries: ["Diffusers", "PyTorch"],
    providers: ["Replicate"]
  },
  {
    name: "Stable-Video-Diffusion-XT",
    repo: "stabilityai/stable-video-diffusion-img2vid-xt-1-1",
    params_b: 3, // Approx effective
    type: "Video",
    license: "STABILITY AI VIDEO LICENSE",
    formats: ["Safetensors", "ONNX"],
    backend: "pytorch",
    quantization: "FP16",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "x86",
      ram_gb: 16,
      gpu_vram_gb: 10
    },
    tasks: ["text-to-video", "image-to-video"],
    pinokio: true,
    text_to_video_prompt: false,
    text_to_image_prompt: false,
    description: "Stability AI's SVD-XT. The standard for local video generation, converting static images to smooth video.",
    publisher: "Stability AI",
    provider: "Stability AI",
    libraries: ["Diffusers", "ONNX"],
    providers: ["Replicate", "Fireworks"]
  },

  // --- Provider Specific Models (NVIDIA, Arcee, Meituan, Supertone) ---
  {
    name: "NVIDIA-Llama-3.1-Nemotron-70B",
    repo: "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF",
    params_b: 70,
    type: "LLM",
    license: "Llama Community",
    formats: ["GGUF", "Safetensors"],
    backend: "gguf",
    quantization: "Q4_K_M",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "x86",
      ram_gb: 64,
      gpu_vram_gb: 40
    },
    tasks: ["chat", "qa", "reasoning", "coding"],
    pinokio: false,
    text_to_video_prompt: true,
    text_to_image_prompt: true,
    description: "NVIDIA's optimized version of Llama 3.1 70B. Exceptional for complex reasoning and enterprise tasks.",
    publisher: "NVIDIA",
    provider: "NVIDIA",
    libraries: ["TensorRT-LLM", "Transformers"],
    providers: ["NVIDIA NIM", "HuggingFace"]
  },
  {
    name: "Arcee-Spark",
    repo: "arcee-ai/Arcee-Spark",
    params_b: 7, 
    type: "SLM",
    license: "Apache 2.0",
    formats: ["GGUF"],
    backend: "gguf",
    quantization: "Q4_K_M",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 8,
      gpu_vram_gb: 6
    },
    tasks: ["domain-adaptation", "chat", "qa"],
    pinokio: false,
    text_to_video_prompt: false,
    text_to_image_prompt: false,
    description: "A specialized 7B model from Arcee AI, designed for efficient domain adaptation and high-quality outputs.",
    publisher: "Arcee AI",
    provider: "Arcee AI",
    libraries: ["MergeKit", "Transformers"],
    providers: ["Arcee Cloud"]
  },
  {
    name: "Meituan-YOLOv6-L",
    repo: "meituan/YOLOv6",
    params_b: 0.05, 
    type: "Vision",
    license: "GPL-3.0",
    formats: ["ONNX"],
    backend: "onnx",
    quantization: "FP16",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 4,
      gpu_vram_gb: 2
    },
    tasks: ["object-detection", "vision"],
    pinokio: true,
    text_to_video_prompt: false,
    text_to_image_prompt: false,
    description: "YOLOv6 by Meituan. A single-stage object detection model dedicated to industrial applications.",
    publisher: "Meituan",
    provider: "Meituan",
    libraries: ["ONNX", "PyTorch"],
    providers: []
  },
  {
    name: "Supertone-Voice-v1",
    repo: "supertone/voice-v1-gguf", 
    params_b: 1, 
    type: "Audio",
    license: "Proprietary (Demo)", 
    formats: ["GGUF"],
    backend: "gguf",
    quantization: "FP16",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "x86",
      ram_gb: 4,
      gpu_vram_gb: 2
    },
    tasks: ["audio", "text-to-speech"],
    pinokio: false,
    text_to_video_prompt: false,
    text_to_image_prompt: false,
    description: "A representative audio model from Supertone, optimized for high-quality voice synthesis.",
    publisher: "Supertone",
    provider: "Supertone",
    libraries: ["GGUF"],
    providers: []
  },

  // --- General GGUF Models (Existing) ---
  {
    name: "OLMo-2-1124-7B-Instruct",
    repo: "allenai/OLMo-2-1124-7B-Instruct-GGUF",
    params_b: 7,
    type: "LLM",
    license: "Apache 2.0",
    formats: ["GGUF"],
    backend: "gguf",
    quantization: "Q4_K_M",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 8,
      gpu_vram_gb: 6
    },
    tasks: ["chat", "qa", "coding", "science"],
    pinokio: true,
    text_to_video_prompt: true,
    text_to_image_prompt: true,
    description: "State-of-the-art open language model by AllenAI, optimized for instruction following and science reasoning.",
    publisher: "AllenAI",
    provider: "AllenAI",
    libraries: ["GGUF", "llama.cpp"],
    providers: []
  },
  {
    name: "OLMoE-1.7B",
    repo: "allenai/OLMoE-1B-7B-0924-Instruct-GGUF",
    params_b: 1.7,
    type: "MoE",
    license: "Apache 2.0",
    formats: ["GGUF"],
    backend: "gguf",
    quantization: "Q4_K_M",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 4,
      gpu_vram_gb: 2
    },
    tasks: ["chat", "qa"],
    pinokio: true,
    text_to_video_prompt: false,
    text_to_image_prompt: false,
    description: "Efficient Mixture-of-Experts model, offering high performance with low compute requirements.",
    publisher: "AllenAI",
    provider: "AllenAI",
    libraries: ["GGUF", "llama.cpp"],
    providers: []
  },
  {
    name: "Phi-3-Mini-3.8B",
    repo: "microsoft/Phi-3-mini-4k-instruct-gguf",
    params_b: 3.8,
    type: "LLM",
    license: "MIT",
    formats: ["GGUF"],
    backend: "gguf",
    quantization: "Q4_K_M",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 8,
      gpu_vram_gb: 4
    },
    tasks: ["chat", "qa", "coding", "reasoning"],
    pinokio: true,
    text_to_video_prompt: true,
    text_to_image_prompt: true,
    description: "Microsoft's highly capable small language model, rivaling much larger models in reasoning.",
    publisher: "Microsoft",
    provider: "Microsoft",
    libraries: ["GGUF", "llama.cpp"],
    providers: ["Azure AI"]
  },
  {
    name: "Phi-3-Vision",
    repo: "microsoft/Phi-3-vision-128k-instruct-gguf",
    params_b: 4.2,
    type: "Multimodal",
    license: "MIT",
    formats: ["GGUF"],
    backend: "gguf",
    quantization: "Q4_K_M",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 12,
      gpu_vram_gb: 6
    },
    tasks: ["chat", "qa", "vision", "ocr"],
    pinokio: true,
    text_to_video_prompt: false,
    text_to_image_prompt: false,
    description: "Multimodal version of Phi-3 capable of reasoning over images, charts, and diagrams.",
    publisher: "Microsoft",
    provider: "Microsoft",
    libraries: ["GGUF", "llama.cpp"],
    providers: ["Azure AI"]
  },
  {
    name: "Qwen2.5-7B-Instruct",
    repo: "Qwen/Qwen2.5-7B-Instruct-GGUF",
    params_b: 7,
    type: "LLM",
    license: "Apache 2.0",
    formats: ["GGUF"],
    backend: "gguf",
    quantization: "Q4_K_M",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 16,
      gpu_vram_gb: 6
    },
    tasks: ["chat", "qa", "coding", "reasoning", "multilingual"],
    pinokio: true,
    text_to_video_prompt: true,
    text_to_image_prompt: true,
    description: "Alibaba's latest 7B powerhouse, excellent at coding and math, outperforming many larger models.",
    publisher: "Qwen",
    provider: "Alibaba Cloud",
    libraries: ["GGUF", "llama.cpp"],
    providers: ["Alibaba Cloud"]
  },
  {
    name: "Mistral-7B-Instruct-v0.3",
    repo: "mistralai/Mistral-7B-Instruct-v0.3-GGUF",
    params_b: 7,
    type: "LLM",
    license: "Apache 2.0",
    formats: ["GGUF"],
    backend: "gguf",
    quantization: "Q4_K_M",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 16,
      gpu_vram_gb: 6
    },
    tasks: ["chat", "qa", "creative"],
    pinokio: true,
    text_to_video_prompt: true,
    text_to_image_prompt: true,
    description: "The classic highly efficient 7B model from Mistral AI, updated for better instruction following.",
    publisher: "Mistral AI",
    provider: "Mistral AI",
    libraries: ["GGUF", "llama.cpp"],
    providers: ["Mistral AI", "Groq", "Together AI"]
  },
  {
    name: "Mistral-Nemo-12B",
    repo: "mistralai/Mistral-Nemo-Instruct-2407-GGUF",
    params_b: 12,
    type: "LLM",
    license: "Apache 2.0",
    formats: ["GGUF"],
    backend: "gguf",
    quantization: "Q4_K_M",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 24,
      gpu_vram_gb: 10
    },
    tasks: ["chat", "qa", "coding", "reasoning"],
    pinokio: true,
    text_to_video_prompt: true,
    text_to_image_prompt: true,
    description: "A collaboration between NVIDIA and Mistral AI, offering 12B parameters with a massive 128k context window.",
    publisher: "Mistral AI / NVIDIA",
    provider: "Mistral AI",
    libraries: ["GGUF", "llama.cpp"],
    providers: ["Mistral AI", "NVIDIA NIM"]
  },
  {
    name: "WizardLM-2-7B",
    repo: "microsoft/WizardLM-2-7B-GGUF",
    params_b: 7,
    type: "LLM",
    license: "Apache 2.0",
    formats: ["GGUF"],
    backend: "gguf",
    quantization: "Q4_K_M",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 16,
      gpu_vram_gb: 6
    },
    tasks: ["chat", "qa", "coding"],
    pinokio: false,
    text_to_video_prompt: true,
    text_to_image_prompt: true,
    description: "Microsoft's WizardLM-2, known for high performance on complex instruction following benchmarks.",
    publisher: "Microsoft",
    provider: "Microsoft",
    libraries: ["GGUF", "llama.cpp"],
    providers: ["Azure AI"]
  },
  {
    name: "Gemma-2-2B-Instruct",
    repo: "google/gemma-2-2b-it-GGUF",
    params_b: 2.6,
    type: "LLM",
    license: "Gemma",
    formats: ["GGUF"],
    backend: "gguf",
    quantization: "Q4_K_M",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 8,
      gpu_vram_gb: 3
    },
    tasks: ["chat", "qa"],
    pinokio: true,
    text_to_video_prompt: false,
    text_to_image_prompt: false,
    description: "Google's lightweight open model, delivering outsized performance for its small footprint.",
    publisher: "Google",
    provider: "Google",
    libraries: ["GGUF", "llama.cpp", "KerasHub"],
    providers: ["Google Cloud"]
  },
  {
    name: "DeepSeek-R1-Distill-Llama-8B",
    repo: "deepseek-ai/DeepSeek-R1-Distill-Llama-8B-GGUF",
    params_b: 8,
    type: "LLM",
    license: "MIT",
    formats: ["GGUF"],
    backend: "gguf",
    quantization: "Q4_K_M",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 16,
      gpu_vram_gb: 7
    },
    tasks: ["chat", "qa", "coding", "math", "reasoning"],
    pinokio: true,
    text_to_video_prompt: true,
    text_to_image_prompt: true,
    description: "Distilled version of DeepSeek R1, achieving massive reasoning capabilities in a manageable size.",
    publisher: "DeepSeek",
    provider: "DeepSeek",
    libraries: ["GGUF", "llama.cpp"],
    providers: ["DeepSeek", "Together AI", "Fireworks"]
  },
  {
    name: "DeepSeek-R1-Distill-Qwen-32B",
    repo: "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B-GGUF",
    params_b: 32,
    type: "LLM",
    license: "MIT",
    formats: ["GGUF"],
    backend: "gguf",
    quantization: "Q4_K_M",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "x86",
      ram_gb: 32,
      gpu_vram_gb: 20
    },
    tasks: ["chat", "qa", "coding", "math", "reasoning"],
    pinokio: true,
    text_to_video_prompt: true,
    text_to_image_prompt: true,
    description: "Highly capable distilled model from DeepSeek R1, optimized for reasoning and complex tasks. Requires substantial VRAM.",
    publisher: "DeepSeek",
    provider: "DeepSeek",
    libraries: ["GGUF", "llama.cpp"],
    providers: ["DeepSeek", "Together AI", "Fireworks"]
  },
  {
    name: "Mistral-Small-24B-Instruct-2501",
    repo: "mistralai/Mistral-Small-24B-Instruct-2501-GGUF",
    params_b: 24,
    type: "LLM",
    license: "Apache 2.0",
    formats: ["GGUF"],
    backend: "gguf",
    quantization: "Q4_K_M",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 24,
      gpu_vram_gb: 16
    },
    tasks: ["chat", "qa", "coding", "reasoning"],
    pinokio: true,
    text_to_video_prompt: true,
    text_to_image_prompt: true,
    description: "Mistral's efficient 24B model, offering a sweet spot between performance and resource usage.",
    publisher: "Mistral AI",
    provider: "Mistral AI",
    libraries: ["GGUF", "llama.cpp"],
    providers: ["Mistral AI", "Groq"]
  }
];

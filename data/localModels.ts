
import { LocalModelDB } from '../types';

export const MODEL_DB: LocalModelDB[] = [
  // --- Scientific & Research Models ---
  {
    name: "SciBERT-Scivocab-Uncased",
    repo: "allenai/scibert_scivocab_uncased",
    params_b: 0.11, // 110M
    type: "Embedding",
    license: "Apache 2.0",
    formats: ["Transformers", "ONNX"],
    backend: "pytorch",
    quantization: "FP32",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 4,
      gpu_vram_gb: 1
    },
    tasks: ["science", "research", "classification", "embedding"],
    pinokio: false,
    text_to_video_prompt: false,
    text_to_image_prompt: false,
    description: "A BERT model trained on scientific text. Excellent for classification and embedding of research papers.",
    publisher: "AllenAI",
    provider: "AllenAI",
    quality: "Excellent",
    libraries: ["Transformers", "ONNX"],
    providers: ["HuggingFace"]
  },
  {
    name: "SciGPT2",
    repo: "feradauto/SciGPT2",
    params_b: 0.12,
    type: "LLM",
    license: "MIT",
    formats: ["Transformers"],
    backend: "pytorch",
    quantization: "FP32",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 4,
      gpu_vram_gb: 2
    },
    tasks: ["science", "research", "text-generation"],
    pinokio: false,
    text_to_video_prompt: false,
    text_to_image_prompt: false,
    description: "GPT-2 model fine-tuned on scientific papers for text generation, capturing scientific discourse patterns.",
    publisher: "Feradauto",
    provider: "Feradauto",
    quality: "Good",
    libraries: ["Transformers", "PyTorch"],
    providers: ["HuggingFace"]
  },
  {
    name: "CATTS-TLDR",
    repo: "neulab/catts-406m", 
    params_b: 0.4,
    type: "Summarization",
    license: "Apache 2.0",
    formats: ["Transformers"],
    backend: "pytorch",
    quantization: "FP32",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 4,
      gpu_vram_gb: 2
    },
    tasks: ["summarization", "science", "research"],
    pinokio: false,
    text_to_video_prompt: false,
    text_to_image_prompt: false,
    description: "Extreme summarization of scientific documents (TLDR). Generates concise summaries of papers.",
    publisher: "NeuLab",
    provider: "NeuLab",
    quality: "Very Good",
    libraries: ["Transformers", "PyTorch"],
    providers: ["HuggingFace"]
  },
  {
    name: "SciNewsBERT",
    repo: "bit-ml/SciNewsBERT",
    params_b: 0.11,
    type: "Embedding",
    license: "MIT",
    formats: ["Transformers"],
    backend: "pytorch",
    quantization: "FP32",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 4,
      gpu_vram_gb: 1
    },
    tasks: ["classification", "fact-checking", "science"],
    pinokio: false,
    text_to_video_prompt: false,
    text_to_image_prompt: false,
    description: "Detecting and contextualizing scientific claims for assisting manual fact-checking.",
    publisher: "BIT-ML",
    provider: "BIT-ML",
    quality: "Good",
    libraries: ["Transformers", "PyTorch"],
    providers: ["HuggingFace"]
  },
  {
    name: "ScholarBERT-Large",
    repo: "globuslabs/ScholarBERT",
    params_b: 0.77,
    type: "Embedding",
    license: "CC-BY-NC-4.0",
    formats: ["Transformers"],
    backend: "pytorch",
    quantization: "FP32",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 8,
      gpu_vram_gb: 2
    },
    tasks: ["science", "research", "embedding"],
    pinokio: false,
    text_to_video_prompt: false,
    text_to_image_prompt: false,
    description: "Large masked language model for science, trained on a massive corpus of 75M+ articles.",
    publisher: "Globus Labs",
    provider: "Globus Labs",
    quality: "Excellent",
    libraries: ["Transformers", "PyTorch"],
    providers: ["HuggingFace"]
  },
  {
    name: "Galactica-1.3B",
    repo: "facebook/galactica-1.3b",
    params_b: 1.3,
    type: "LLM",
    license: "CC-BY-NC-4.0",
    formats: ["GGUF"],
    backend: "gguf",
    quantization: "Q4_K_M",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 4,
      gpu_vram_gb: 2
    },
    tasks: ["science", "research", "physics", "formula", "chemistry"],
    pinokio: true,
    text_to_video_prompt: false,
    text_to_image_prompt: false,
    description: "Meta's scientific model. Good for LaTeX generation, scientific referencing, and formula creation.",
    publisher: "Meta",
    provider: "Meta",
    quality: "Very Good",
    libraries: ["GGUF", "llama.cpp"],
    providers: []
  },
  {
    name: "SciDFM-18.2B",
    repo: "OpenDFM/SciDFM-18.2B",
    params_b: 18.2,
    type: "MoE",
    license: "Apache 2.0",
    formats: ["Transformers"],
    backend: "pytorch",
    quantization: "BF16",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 32,
      gpu_vram_gb: 24
    },
    tasks: ["science", "research", "chat"],
    pinokio: false,
    text_to_video_prompt: false,
    text_to_image_prompt: false,
    description: "A Large Language Model with Mixture-of-Experts for Science.",
    publisher: "OpenDFM",
    provider: "OpenDFM",
    quality: "Excellent",
    libraries: ["Transformers", "PyTorch"],
    providers: ["HuggingFace"]
  },

  // --- Apple Silicon MLX Models ---
  {
    name: "Llama-3.2-3B-Instruct-MLX",
    repo: "mlx-community/Llama-3.2-3B-Instruct-4bit",
    params_b: 3.2,
    type: "LLM",
    license: "Llama Community",
    formats: ["Safetensors"],
    backend: "mlx",
    quantization: "4bit",
    apple_silicon_optimized: true,
    min_hardware: {
      cpu: "Apple Silicon",
      ram_gb: 8,
      gpu_vram_gb: 3
    },
    tasks: ["chat", "qa", "creative"],
    pinokio: false,
    text_to_video_prompt: true,
    text_to_image_prompt: true,
    description: "Highly efficient Llama 3.2 3B model optimized for Apple Silicon via MLX. Blazing fast inference on M1/M2/M3.",
    publisher: "Meta / MLX Community",
    provider: "Meta",
    quality: "Very Good",
    libraries: ["MLX", "Safetensors", "Transformers"],
    providers: ["Groq", "Together AI", "Fireworks"]
  },
  {
    name: "DeepSeek-Coder-V2-Lite-MLX",
    repo: "mlx-community/DeepSeek-Coder-V2-Lite-Instruct-4bit-mlx",
    params_b: 16,
    type: "MoE",
    license: "DeepSeek License",
    formats: ["Safetensors"],
    backend: "mlx",
    quantization: "4bit",
    apple_silicon_optimized: true,
    min_hardware: {
      cpu: "Apple Silicon",
      ram_gb: 16,
      gpu_vram_gb: 10
    },
    tasks: ["coding", "chat", "reasoning"],
    pinokio: false,
    text_to_video_prompt: false,
    text_to_image_prompt: false,
    description: "State-of-the-art coding model (MoE) optimized for Mac. Rivals GPT-4 Turbo in coding tasks.",
    publisher: "DeepSeek / MLX Community",
    provider: "DeepSeek",
    quality: "Exceptional",
    libraries: ["MLX", "Safetensors"],
    providers: ["DeepSeek", "Together AI"]
  },
  {
    name: "Phi-3.5-Mini-Instruct-MLX",
    repo: "mlx-community/Phi-3.5-mini-instruct-4bit",
    params_b: 3.8,
    type: "LLM",
    license: "MIT",
    formats: ["Safetensors"],
    backend: "mlx",
    quantization: "4bit",
    apple_silicon_optimized: true,
    min_hardware: {
      cpu: "Apple Silicon",
      ram_gb: 8,
      gpu_vram_gb: 4
    },
    tasks: ["chat", "qa", "reasoning"],
    pinokio: false,
    text_to_video_prompt: true,
    text_to_image_prompt: true,
    description: "Microsoft's Phi-3.5 Mini, perfectly optimized for Apple Silicon. Exceptional reasoning for its size.",
    publisher: "Microsoft / MLX Community",
    provider: "Microsoft",
    quality: "Excellent",
    libraries: ["MLX", "Transformers"],
    providers: ["Azure AI", "HuggingFace"]
  },
  {
    name: "Mistral-Nemo-12B-MLX",
    repo: "mlx-community/Mistral-Nemo-Instruct-2407-4bit",
    params_b: 12,
    type: "LLM",
    license: "Apache 2.0",
    formats: ["Safetensors"],
    backend: "mlx",
    quantization: "4bit",
    apple_silicon_optimized: true,
    min_hardware: {
      cpu: "Apple Silicon",
      ram_gb: 16,
      gpu_vram_gb: 8
    },
    tasks: ["chat", "qa", "creative", "coding"],
    pinokio: false,
    text_to_video_prompt: true,
    text_to_image_prompt: true,
    description: "Mistral Nemo 12B running natively on MLX. Large context window and strong general performance.",
    publisher: "Mistral AI / MLX Community",
    provider: "Mistral AI",
    quality: "Excellent",
    libraries: ["MLX", "Transformers"],
    providers: ["Mistral AI", "Together AI", "Groq"]
  },

  // --- DeepSeek R1 Series (Reasoning) ---
  {
    name: "DeepSeek-R1-Distill-Qwen-1.5B",
    repo: "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
    params_b: 1.5,
    type: "Reasoning",
    license: "MIT",
    formats: ["GGUF", "Safetensors"],
    backend: "gguf",
    quantization: "Q4_K_M",
    apple_silicon_optimized: true,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 4,
      gpu_vram_gb: 2
    },
    tasks: ["reasoning", "math", "coding", "chat"],
    pinokio: true,
    text_to_video_prompt: true,
    text_to_image_prompt: true,
    description: "Ultra-efficient distilled reasoning model. Incredible performance for its size on logic/math.",
    publisher: "DeepSeek",
    provider: "DeepSeek",
    quality: "Very Good",
    libraries: ["GGUF", "llama.cpp", "MLX"],
    providers: ["DeepSeek"]
  },
  {
    name: "DeepSeek-R1-Distill-Qwen-14B",
    repo: "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
    params_b: 14,
    type: "Reasoning",
    license: "MIT",
    formats: ["GGUF", "Safetensors"],
    backend: "gguf",
    quantization: "Q4_K_M",
    apple_silicon_optimized: true,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 16,
      gpu_vram_gb: 10
    },
    tasks: ["reasoning", "math", "coding", "chat"],
    pinokio: true,
    text_to_video_prompt: true,
    text_to_image_prompt: true,
    description: "The sweet spot for local reasoning. Outperforms many larger models on complex tasks.",
    publisher: "DeepSeek",
    provider: "DeepSeek",
    quality: "Exceptional",
    libraries: ["GGUF", "llama.cpp", "MLX"],
    providers: ["DeepSeek"]
  },
  {
    name: "DeepSeek-R1-Distill-Llama-70B",
    repo: "deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
    params_b: 70,
    type: "Reasoning",
    license: "Llama Community",
    formats: ["GGUF", "Safetensors"],
    backend: "gguf",
    quantization: "Q4_K_M",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 64,
      gpu_vram_gb: 40
    },
    tasks: ["reasoning", "math", "coding", "research"],
    pinokio: true,
    text_to_video_prompt: true,
    text_to_image_prompt: true,
    description: "Massive reasoning capability distilled from DeepSeek-V3 into Llama 70B architecture. Enterprise grade.",
    publisher: "DeepSeek",
    provider: "DeepSeek",
    quality: "Exceptional",
    libraries: ["GGUF", "llama.cpp", "vLLM"],
    providers: ["DeepSeek", "Groq"]
  },

  // --- Qwen 2.5 Family ---
  {
    name: "Qwen2.5-32B-Instruct",
    repo: "Qwen/Qwen2.5-32B-Instruct-GGUF",
    params_b: 32,
    type: "LLM",
    license: "Apache 2.0",
    formats: ["GGUF"],
    backend: "gguf",
    quantization: "Q4_K_M",
    apple_silicon_optimized: true,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 32,
      gpu_vram_gb: 20
    },
    tasks: ["chat", "coding", "multilingual", "creative"],
    pinokio: true,
    text_to_video_prompt: true,
    text_to_image_prompt: true,
    description: "The 'Golden Ratio' model. Fits on 24GB GPUs (3090/4090) with Q4 quantization. Beats Llama 3 70B in many benchmarks.",
    publisher: "Qwen",
    provider: "Alibaba Cloud",
    quality: "Excellent",
    libraries: ["GGUF", "llama.cpp"],
    providers: ["Alibaba Cloud"]
  },
  {
    name: "Qwen2.5-Coder-32B-Instruct",
    repo: "Qwen/Qwen2.5-Coder-32B-Instruct-GGUF",
    params_b: 32,
    type: "Code",
    license: "Apache 2.0",
    formats: ["GGUF"],
    backend: "gguf",
    quantization: "Q4_K_M",
    apple_silicon_optimized: true,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 32,
      gpu_vram_gb: 20
    },
    tasks: ["coding", "reasoning"],
    pinokio: true,
    text_to_video_prompt: false,
    text_to_image_prompt: false,
    description: "State-of-the-art open code model. Rivals GPT-4o in coding tasks.",
    publisher: "Qwen",
    provider: "Alibaba Cloud",
    quality: "Exceptional",
    libraries: ["GGUF", "llama.cpp"],
    providers: ["Alibaba Cloud"]
  },
  {
    name: "QwQ-32B-Preview",
    repo: "Qwen/QwQ-32B-Preview-GGUF",
    params_b: 32,
    type: "Reasoning",
    license: "Apache 2.0",
    formats: ["GGUF"],
    backend: "gguf",
    quantization: "Q4_K_M",
    apple_silicon_optimized: true,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 32,
      gpu_vram_gb: 20
    },
    tasks: ["reasoning", "math", "science"],
    pinokio: true,
    text_to_video_prompt: false,
    text_to_image_prompt: false,
    description: "Qwen's answer to OpenAI o1. Specialized in long-chain-of-thought reasoning.",
    publisher: "Qwen",
    provider: "Alibaba Cloud",
    quality: "Excellent",
    libraries: ["GGUF", "llama.cpp"],
    providers: ["Alibaba Cloud"]
  },
  {
    name: "Qwen2.5-VL-3B-Instruct",
    repo: "Qwen/Qwen2.5-VL-3B-Instruct-GGUF",
    params_b: 3,
    type: "Multimodal",
    license: "Apache 2.0",
    formats: ["GGUF"],
    backend: "gguf",
    quantization: "Q4_K_M",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 8,
      gpu_vram_gb: 4
    },
    tasks: ["vision", "ocr", "video-analysis"],
    pinokio: true,
    text_to_video_prompt: false,
    text_to_image_prompt: false,
    description: "Highly capable small vision-language model. Can handle high-res images and video understanding.",
    publisher: "Qwen",
    provider: "Alibaba Cloud",
    quality: "Very Good",
    libraries: ["GGUF", "llama.cpp"],
    providers: ["Alibaba Cloud"]
  },

  // --- Meta Llama 3.X Family ---
  {
    name: "Llama-3.3-70B-Instruct",
    repo: "meta-llama/Llama-3.3-70B-Instruct",
    params_b: 70,
    type: "LLM",
    license: "Llama Community",
    formats: ["GGUF", "Safetensors"],
    backend: "gguf",
    quantization: "Q4_K_M",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 64,
      gpu_vram_gb: 40
    },
    tasks: ["chat", "qa", "reasoning", "creative"],
    pinokio: true,
    text_to_video_prompt: true,
    text_to_image_prompt: true,
    description: "The refined 70B model from Meta. Excellent general purpose capabilities.",
    publisher: "Meta",
    provider: "Meta",
    quality: "Exceptional",
    libraries: ["GGUF", "llama.cpp"],
    providers: ["Groq", "Together AI"]
  },
  {
    name: "Llama-3.2-11B-Vision-Instruct",
    repo: "meta-llama/Llama-3.2-11B-Vision-Instruct",
    params_b: 11,
    type: "Multimodal",
    license: "Llama Community",
    formats: ["GGUF", "Safetensors"],
    backend: "gguf",
    quantization: "Q4_K_M",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 16,
      gpu_vram_gb: 10
    },
    tasks: ["vision", "ocr", "chat"],
    pinokio: true,
    text_to_video_prompt: false,
    text_to_image_prompt: false,
    description: "Meta's mid-sized vision model. Good balance of text and image reasoning.",
    publisher: "Meta",
    provider: "Meta",
    quality: "Excellent",
    libraries: ["GGUF", "llama.cpp"],
    providers: ["Groq"]
  },

  // --- Microsoft ---
  {
    name: "Phi-4",
    repo: "microsoft/phi-4",
    params_b: 14,
    type: "Reasoning",
    license: "MIT",
    formats: ["GGUF", "Safetensors"],
    backend: "gguf",
    quantization: "Q4_K_M",
    apple_silicon_optimized: true,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 16,
      gpu_vram_gb: 10
    },
    tasks: ["reasoning", "math", "coding"],
    pinokio: true,
    text_to_video_prompt: false,
    text_to_image_prompt: false,
    description: "Microsoft's latest reasoning model trained on synthetic data. Punches way above its weight.",
    publisher: "Microsoft",
    provider: "Microsoft",
    quality: "Very Good",
    libraries: ["GGUF", "llama.cpp", "ONNX"],
    providers: ["Azure AI"]
  },

  // --- Mistral ---
  {
    name: "Ministral-8B-Instruct",
    repo: "mistralai/Ministral-8B-Instruct-2410",
    params_b: 8,
    type: "LLM",
    license: "Apache 2.0",
    formats: ["GGUF"],
    backend: "gguf",
    quantization: "Q4_K_M",
    apple_silicon_optimized: true,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 8,
      gpu_vram_gb: 6
    },
    tasks: ["chat", "edge"],
    pinokio: true,
    text_to_video_prompt: true,
    text_to_image_prompt: true,
    description: "Mistral's powerful edge model. Designed for low-latency and on-device use.",
    publisher: "Mistral AI",
    provider: "Mistral AI",
    quality: "Good",
    libraries: ["GGUF", "llama.cpp"],
    providers: ["Mistral AI"]
  },
  {
    name: "Pixtral-12B-2409",
    repo: "mistralai/Pixtral-12B-2409",
    params_b: 12,
    type: "Multimodal",
    license: "Apache 2.0",
    formats: ["GGUF"],
    backend: "gguf",
    quantization: "Q4_K_M",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 16,
      gpu_vram_gb: 10
    },
    tasks: ["vision", "ocr", "chat"],
    pinokio: true,
    text_to_video_prompt: false,
    text_to_image_prompt: false,
    description: "Mistral's first multimodal model. Built on Nemo 12B with a 400M vision adapter.",
    publisher: "Mistral AI",
    provider: "Mistral AI",
    quality: "Excellent",
    libraries: ["GGUF", "llama.cpp"],
    providers: ["Mistral AI"]
  },

  // --- Google Gemma ---
  {
    name: "Gemma-2-9B-Instruct",
    repo: "google/gemma-2-9b-it",
    params_b: 9,
    type: "LLM",
    license: "Gemma",
    formats: ["GGUF"],
    backend: "gguf",
    quantization: "Q4_K_M",
    apple_silicon_optimized: true,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 12,
      gpu_vram_gb: 8
    },
    tasks: ["chat", "qa", "creative"],
    pinokio: true,
    text_to_video_prompt: true,
    text_to_image_prompt: true,
    description: "Google's open model with exceptional performance for its size class.",
    publisher: "Google",
    provider: "Google",
    quality: "Very Good",
    libraries: ["GGUF", "llama.cpp"],
    providers: ["Google Cloud"]
  },
  {
    name: "Gemma-2-27B-Instruct",
    repo: "google/gemma-2-27b-it",
    params_b: 27,
    type: "LLM",
    license: "Gemma",
    formats: ["GGUF"],
    backend: "gguf",
    quantization: "Q4_K_M",
    apple_silicon_optimized: true,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 32,
      gpu_vram_gb: 18
    },
    tasks: ["chat", "reasoning", "coding"],
    pinokio: true,
    text_to_video_prompt: true,
    text_to_image_prompt: true,
    description: "A robust mid-sized model from Google, comparable to Llama 3 70B in many aspects.",
    publisher: "Google",
    provider: "Google",
    quality: "Excellent",
    libraries: ["GGUF", "llama.cpp"],
    providers: ["Google Cloud"]
  },

  // --- Nous Research / Cognitive Computations ---
  {
    name: "Hermes-3-Llama-3.1-8B",
    repo: "NousResearch/Hermes-3-Llama-3.1-8B",
    params_b: 8,
    type: "LLM",
    license: "Apache 2.0",
    formats: ["GGUF"],
    backend: "gguf",
    quantization: "Q4_K_M",
    apple_silicon_optimized: true,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 8,
      gpu_vram_gb: 6
    },
    tasks: ["roleplay", "agent", "creative"],
    pinokio: true,
    text_to_video_prompt: true,
    text_to_image_prompt: true,
    description: "The latest Hermes. Unlocked, highly steerable, and great for roleplay and agentic tasks.",
    publisher: "Nous Research",
    provider: "Nous Research",
    quality: "Good",
    libraries: ["GGUF", "llama.cpp"],
    providers: ["Together AI"]
  },
  {
    name: "Dolphin-3.0-Mistral-24B",
    repo: "cognitivecomputations/Dolphin3.0-Mistral-24B",
    params_b: 24,
    type: "LLM",
    license: "Apache 2.0",
    formats: ["GGUF"],
    backend: "gguf",
    quantization: "Q4_K_M",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 24,
      gpu_vram_gb: 16
    },
    tasks: ["roleplay", "coding", "uncensored"],
    pinokio: true,
    text_to_video_prompt: true,
    text_to_image_prompt: true,
    description: "An uncensored, highly compliant model based on Mistral Small 3. Excellent for creative writing.",
    publisher: "Cognitive Computations",
    provider: "Cognitive Computations",
    quality: "Very Good",
    libraries: ["GGUF", "llama.cpp"],
    providers: []
  },

  // --- Image Generation ---
  {
    name: "FLUX.1-Schnell",
    repo: "black-forest-labs/FLUX.1-schnell",
    params_b: 12,
    type: "Image",
    license: "Apache 2.0",
    formats: ["Safetensors", "GGUF"],
    backend: "pytorch",
    quantization: "FP8",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "x86",
      ram_gb: 16,
      gpu_vram_gb: 12
    },
    tasks: ["text-to-image"],
    pinokio: true,
    text_to_video_prompt: false,
    text_to_image_prompt: false,
    description: "The fastest version of FLUX.1. State-of-the-art open image generation.",
    publisher: "Black Forest Labs",
    provider: "Black Forest Labs",
    quality: "Exceptional",
    libraries: ["Diffusers", "ComfyUI"],
    providers: ["Replicate", "Fal.ai"]
  },

  // --- Other High Quality Models ---
  {
    name: "Yi-1.5-34B-Chat",
    repo: "01-ai/Yi-1.5-34B-Chat",
    params_b: 34,
    type: "LLM",
    license: "Apache 2.0",
    formats: ["GGUF"],
    backend: "gguf",
    quantization: "Q4_K_M",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 32,
      gpu_vram_gb: 24
    },
    tasks: ["chat", "coding", "creative"],
    pinokio: true,
    text_to_video_prompt: true,
    text_to_image_prompt: true,
    description: "An upgraded version of Yi. Strong performance in chat and coding benchmarks.",
    publisher: "01.AI",
    provider: "01.AI",
    quality: "Excellent",
    libraries: ["GGUF", "llama.cpp"],
    providers: []
  },
  {
    name: "InternVL2-8B",
    repo: "OpenGVLab/InternVL2-8B",
    params_b: 8,
    type: "Multimodal",
    license: "Apache 2.0",
    formats: ["Safetensors"],
    backend: "pytorch",
    quantization: "BF16",
    apple_silicon_optimized: false,
    min_hardware: {
      cpu: "x86/ARM",
      ram_gb: 16,
      gpu_vram_gb: 12
    },
    tasks: ["vision", "ocr", "video-analysis"],
    pinokio: false,
    text_to_video_prompt: false,
    text_to_image_prompt: false,
    description: "A powerful multimodal model competing with commercial VLM offerings.",
    publisher: "OpenGVLab",
    provider: "OpenGVLab",
    quality: "Very Good",
    libraries: ["Transformers", "LMDeploy"],
    providers: []
  }
];
